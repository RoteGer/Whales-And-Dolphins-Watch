{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c44f73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import scipy as sc\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64966cb1",
   "metadata": {},
   "source": [
    "### Step 1 : Get all pages that contain tables (and save it into a list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1117a43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "url = \"https://www.dolphinsafari.com/sightings-log/whale-watching-sightings-archive/\" # The first Url of the page\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.content, \"html.parser\") #Bs object\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73834282",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "links=[]\n",
    "linksToPages=[]\n",
    "\n",
    "for a in soup.find_all('a', href=True):\n",
    "    links.append(a['href'])\n",
    "\n",
    "linksToPages=[l for l in links if  'https://www.dolphinsafari.com/sightings-log/sightings' in l or 'https://www.dolphinsafari.com/sightings-log/whale-watching-sightings-log' in l or 'https://www.dolphinsafari.com/sightings-log/dana-point-whale-watching-sightings-log' in l or 'https://www.dolphinsafari.com/sightings-log/2016-whale-watching-sightings/' in l or l== 'https://www.dolphinsafari.com/sightings-log/' ]\n",
    "\n",
    "linksToPages.remove(linksToPages[0])\n",
    "linksToPages.remove(linksToPages[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b963a6b",
   "metadata": {},
   "source": [
    "### step 2 : Getting the tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e54aaa8",
   "metadata": {},
   "source": [
    "###### In next cell we are crawling the information for the years 2002-2018 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44108529",
   "metadata": {},
   "outputs": [],
   "source": [
    "cells = list()\n",
    "Date = list()\n",
    "Bottlenose_Dolphin = list()\n",
    "Common_Dolphin = list()\n",
    "Pacific_White_sided_Dolphin = list()\n",
    "Rissos_Dolphin = list()\n",
    "Whales = list()\n",
    "Other = list()\n",
    "Number_of_trips = list()\n",
    "\n",
    "Date_2 = list()\n",
    "no_of_trips_2 = list()\n",
    "sightings = list()\n",
    "\n",
    "Years = [\"2002\",\"2003\", \"2004\",\"2005\",\"2006\",\"2007\",\"2008\",\"2009\",'2010','2011',\"2012\",\"2013\",\"2014\",\"2015\",\"2016\",\"2017\",\"2018\",\"2019\",\"2020\"]\n",
    "\n",
    "\n",
    "         ### 2002 - 2018\n",
    "for i in range(17): \n",
    "    response1 = requests.get(linksToPages[i])\n",
    "    soup1 = BeautifulSoup(response1.content, \"html.parser\")  # Bs object\n",
    "    table = soup1.find_all(\"tbody\")\n",
    "    table = list(table)[0]\n",
    "    \n",
    "    for rows in table(\"tr\"):  # tr = table rows\n",
    "        cells = rows(\"td\")  # the cells\n",
    "        \n",
    "        if Years[i]=='2003' or Years[i]=='2004':  # In those years Other and Grey Whales switch places\n",
    "            Date.append(cells[0].get_text().strip())  # getting the Dates\n",
    "            Bottlenose_Dolphin.append(cells[1].get_text().strip())\n",
    "            Common_Dolphin.append(cells[2].get_text().strip())\n",
    "            try:Rissos_Dolphin.append(cells[4].get_text().strip())\n",
    "            except:Rissos_Dolphin.append('0')  \n",
    "                \n",
    "            try:Pacific_White_sided_Dolphin.append(cells[3].get_text().strip())\n",
    "            except:Pacific_White_sided_Dolphin.append('0')\n",
    "                \n",
    "            try:Whales.append(cells[6].get_text().strip())\n",
    "            except:Whales.append('0')\n",
    "                \n",
    "            try: Other.append(cells[5].get_text().strip())\n",
    "            except:Other.append('0')\n",
    "                \n",
    "            Number_of_trips.append(np.nan)  # adding Nan values to missing data of number of trips\n",
    "            \n",
    "        elif(Years[i] > '2010' and Years[i] < '2017'):  # between years 2011-2016 number of trips colum was added\n",
    "            Date.append(cells[0].get_text().strip())  # getting the Dates\n",
    "            Number_of_trips.append(cells[1].get_text().strip())\n",
    "            Bottlenose_Dolphin.append(cells[2].get_text().strip())\n",
    "            Common_Dolphin.append(cells[3].get_text().strip())\n",
    "            try:Rissos_Dolphin.append(cells[5].get_text().strip())\n",
    "            except:Rissos_Dolphin.append('0')    \n",
    "\n",
    "            try:Pacific_White_sided_Dolphin.append(cells[4].get_text().strip())\n",
    "            except:Pacific_White_sided_Dolphin.append('0')\n",
    "\n",
    "            try:Whales.append(cells[6].get_text().strip())\n",
    "            except:Whales.append('0')\n",
    "                \n",
    "            try: Other.append(cells[7].get_text().strip())\n",
    "            except:Other.append('0')\n",
    "                \n",
    "        elif (Years[i]=='2002' or (Years[i] < '2011' and Years[i] > '2004')):  # Years 2002 and 2005-2010, years without nomber of trips column\n",
    "            Date.append(cells[0].get_text().strip())  # getting the Dates\n",
    "            Bottlenose_Dolphin.append(cells[1].get_text().strip())\n",
    "            Common_Dolphin.append(cells[2].get_text().strip())\n",
    "            try:Rissos_Dolphin.append(cells[4].get_text().strip())\n",
    "            except:Rissos_Dolphin.append('0')    \n",
    "\n",
    "            try:Pacific_White_sided_Dolphin.append(cells[3].get_text().strip())\n",
    "            except:Pacific_White_sided_Dolphin.append('0')\n",
    "\n",
    "            try:Whales.append(cells[5].get_text().strip())\n",
    "            except:Whales.append('0')\n",
    "            try: Other.append(cells[6].get_text().strip())\n",
    "            except:Other.append('0')\n",
    "            Number_of_trips.append(np.nan)  # adding NAN values to missing data of number of trips\n",
    "        \n",
    "        elif (Years[i] > '2016' and Years[i] < '2019'):  # Years 2017-2018\n",
    "            Date_2.append(cells[0].get_text().strip())  # getting the Dates\n",
    "            no_of_trips_2.append(cells[1].get_text().strip())\n",
    "            sightings.append(cells[2].get_text().strip())  \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958c92aa",
   "metadata": {},
   "source": [
    "###### In next cells we are crawling the information for the years 2019-2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40ae2a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Year 2019\n",
    "    ## in 2019, the records was different so the crawling needed a change\n",
    "    \n",
    "response1 = requests.get(linksToPages[17])\n",
    "soup1 = BeautifulSoup(response1.content, \"html.parser\")  # Bs object\n",
    "table = soup1.find_all(\"tbody\")\n",
    "           \n",
    "for j in range(12):\n",
    "    table2 = list(table)[j]\n",
    "\n",
    "    \n",
    "    for rows in table2(\"tr\"):  # tr = table rows\n",
    "            \n",
    "        cells = rows(\"td\")  # the cells\n",
    "    \n",
    "        Date_2.append(cells[0].get_text().strip())  # getting the Dates\n",
    "        no_of_trips_2.append(cells[1].get_text().strip())\n",
    "        sightings.append(cells[2].get_text().strip()) \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f985e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Year 2020\n",
    "    ## in 2020, the same as in 2019, but April was left out because of covid-19\n",
    "    \n",
    "response1 = requests.get(linksToPages[18])\n",
    "soup1 = BeautifulSoup(response1.content, \"html.parser\")  # Bs object\n",
    "table = soup1.find_all(\"tbody\")\n",
    "           \n",
    "for j in range(11):\n",
    "    table2 = list(table)[j]\n",
    "  \n",
    "    for rows in table2(\"tr\"):  # tr = table rows\n",
    "            \n",
    "        cells = rows(\"td\")  # the cells\n",
    "            \n",
    "        Date_2.append(cells[0].get_text().strip())  # getting the Dates\n",
    "        no_of_trips_2.append(cells[1].get_text().strip())\n",
    "        sightings.append(cells[2].get_text().strip()) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18abaed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Year 2021\n",
    "    ## in 2021, the same as in 2019\n",
    "    \n",
    "response1 = requests.get(linksToPages[19])\n",
    "soup1 = BeautifulSoup(response1.content, \"html.parser\")  # Bs object\n",
    "table = soup1.find_all(\"tbody\")\n",
    "           \n",
    "for j in range(12):\n",
    "    table2 = list(table)[j]\n",
    "\n",
    "    \n",
    "    for rows in table2(\"tr\"):  # tr = table rows\n",
    "            \n",
    "        cells = rows(\"td\")  # the cells\n",
    "    \n",
    "        Date_2.append(cells[0].get_text().strip())  # getting the Dates\n",
    "        no_of_trips_2.append(cells[1].get_text().strip())\n",
    "        sightings.append(cells[2].get_text().strip()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7923b2f",
   "metadata": {},
   "source": [
    "### step 3 : DataFrames\n",
    "#### Creating 2 DataFrames (We have 2 kind of tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5eff0c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2002 - 2016 df\n",
    "df = pd.DataFrame({\"Date\": Date,\"Number of trips\": Number_of_trips , \"Bottlenose Dolphin\":Bottlenose_Dolphin,\n",
    "                   \"Common Dolphin\": Common_Dolphin, \"Pacific White sided Dolphin\":Pacific_White_sided_Dolphin,\n",
    "                   \"Rissos Dolphin\": Rissos_Dolphin,\"Whales\": Whales, \"Other\":Other})\n",
    "\n",
    "# 2017 - 2021 df\n",
    "df_17_21 = pd.DataFrame({\"Date\": Date_2,\"Number of trips\": no_of_trips_2 , \"Sightings\":sightings})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c5a2d3",
   "metadata": {},
   "source": [
    "#### Adding 'Gray Whales Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25e7dcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adding 'Gray Whales' column to 2002-2016\n",
    "\n",
    "Gray_Whale = [0]*(df.shape[0])\n",
    "\n",
    "for i in range (df.shape[0]):  ## Grey whales column\n",
    "    if Whales[i] == \"Gray\" or Whales[i] == \"Grays\":\n",
    "        Gray_Whale[i] = 1\n",
    "            \n",
    "    elif(\"Gray\" in str(Whales[i])):\n",
    "        try: Gray_Whale[i] = re.findall(r'\\d+', Whales[i])[0]\n",
    "        except: Gray_Whale[i] = 1\n",
    "    \n",
    "    if Other[i] == \"Gray\" or Other[i] == \"Grays\":\n",
    "         Gray_Whale[i] = 1\n",
    "    elif(\"Gray\" in str(Other[i])):\n",
    "        try: Gray_Whale[i] = re.findall(r'\\d+', Other[i])[0]   \n",
    "        except: Gray_Whale[i] = 1\n",
    "\n",
    "df = pd.DataFrame({\"Date\": Date,\"Number of trips\": Number_of_trips,\n",
    "                        \"Bottlenose Dolphin\":Bottlenose_Dolphin, \"Common Dolphin\": Common_Dolphin,\n",
    "                        \"Pacific White sided Dolphin\":Pacific_White_sided_Dolphin,\n",
    "                        \"Rissos Dolphin\": Rissos_Dolphin,\"Whales\": Whales,\"Gray Whales\": Gray_Whale, \"Other\":Other})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5455d8dd",
   "metadata": {},
   "source": [
    "#### Extracting the relevant animals from 'Sightings'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1edf4b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Seperating the 'sightings' to individual columns. years 2017-2021\n",
    "\n",
    "sightings_strings = df_17_21['Sightings'].str.split(', ')  # A list of all strings from sightings column\n",
    "\n",
    "##  Creating a DataFrame of 2017-2021 based on the number of days\n",
    "df_len = len(sightings_strings)\n",
    "Whale_17_21 = [0]*df_len\n",
    "bottle_Dolphin_17_21 = [0]*df_len\n",
    "pacific_Dolphin_17_21 = [0]*df_len\n",
    "Riss_dolphin_17_21 = [0]*df_len\n",
    "com_dolphin_17_21 = [0]*df_len\n",
    "other_17_21 = [0]*df_len\n",
    "Gray_whale_17_21 = [0]*df_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1992def",
   "metadata": {},
   "outputs": [],
   "source": [
    " ### Filling the DataFrame with the relevant information from 'Sightings' \n",
    "\n",
    "for i in range (df_17_21.shape[0]):\n",
    "    for j in range (len(sightings_strings[i])):\n",
    "        if ('Gray Whales' in sightings_strings[i][j] or 'Gray Whale' in sightings_strings[i][j]):\n",
    "            Gray_whale_17_21[i] = sightings_strings[i][j]\n",
    "    \n",
    "        if ('Pacific White-sided Dolphins' in sightings_strings[i][j] or 'Pacific White-sided Dolphin' in sightings_strings[i][j]):\n",
    "            pacific_Dolphin_17_21[i] = sightings_strings[i][j]  \n",
    "\n",
    "        if ('Common Dolphins' in sightings_strings[i][j] or 'Common Dolphin' in sightings_strings[i][j]):\n",
    "            com_dolphin_17_21[i]= sightings_strings[i][j]\n",
    "        \n",
    "        if ('Rissoâ€™s Dolphin' in sightings_strings[i][j] or 'Rissoâ€™s Dolphins' in sightings_strings[i][j]):\n",
    "            Riss_dolphin_17_21[i]= sightings_strings[i][j]\n",
    "        \n",
    "        if ('Bottlenose Dolphins' in sightings_strings[i][j] or 'Bottlenose Dolphin' in sightings_strings[i][j]):\n",
    "            bottle_Dolphin_17_21[i]= sightings_strings[i][j]\n",
    "            \n",
    "        if ('whale' in sightings_strings[i][j] or 'Whales' in sightings_strings[i][j]):\n",
    "            Whale_17_21[i] = sightings_strings[i][j]\n",
    "            \n",
    "        if ('Elephant Seal' in sightings_strings[i][j]  or 'Mola' in sightings_strings[i][j] or 'Sharks' in sightings_strings[i][j]):\n",
    "            other_17_21[i]= 1\n",
    "\n",
    "\n",
    "### Building the DataFrame for years 2017-2021            \n",
    "            \n",
    "df_17_21 = pd.DataFrame({\"Date\": Date_2,\"Number of trips\": no_of_trips_2 , \"Bottlenose Dolphin\":bottle_Dolphin_17_21, \"Common Dolphin\":com_dolphin_17_21,\n",
    "                         \"Pacific White sided Dolphin\":pacific_Dolphin_17_21, \"Rissos Dolphin\":Riss_dolphin_17_21, \"Whales\":Whale_17_21,\n",
    "                         \"Gray Whales\": Gray_whale_17_21, \"Other\": other_17_21})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf36542",
   "metadata": {},
   "source": [
    "### step 4 : Final DataFrame\n",
    "## Merging the DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "812dff81",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Merging the 2 DataFrames\n",
    "## now we have 2002-2021\n",
    "\n",
    "frames = [df, df_17_21]\n",
    "  \n",
    "full_data = pd.concat(frames, join='inner',ignore_index=True)\n",
    "full_data.to_csv(\"full_data_02_21.csv\")"
    "full_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4c1d72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
